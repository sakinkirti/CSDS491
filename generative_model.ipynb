{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### E.2 A continuous inference problem"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This past summer, I interned at Surgo Health, a company that is trying to solve healthcare problems by looking and social and behavioral contexts of patients. In doing so, they exploit causal machine learning methods to better understand and classify patients and their actions. Of course, this requires massive datasets, with my manager briefly explaining that they often avoid causal ML on datasets with fewer than 10,000 samples. Because of the lack of datasets that are readily available, part of my internship included building a generative network that would be able to generate causally-linked data across 50+ discrete features. This work ended up being the inspiration behind me taking this course, but left me with a specific question - How might we go about building bayesian networks for continuous features. As far as I could see, doing so with discrete variables was hard enough, but allows one to bin values. Making causal predictions across continous features felt somewhat far-fetched. Therefore, for this exploration, I decided to look into several papers and websites that explain continous bayesian networks.<br><br>\n",
    "Looking ahead in the textbook, I was able to find a section on bayesian networks with continuous variables. It seems that implementing this as a generative model would not be entirely difficult since we can define a causal relationship via a function. Similarly, implementing a mixed model using both continuous and discrete values would mean generating values similarly as done using the continuous model and then adding a discretization step where a threshold is applied to create $n$ distinct classes.<br><br>\n",
    "Below, I have tried to implement a small continuous generative network with three nodes which create a v-structure. A v-structure looks as follows: ![](v-structure.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can define a node $A$, $B$, and $V$ shown above by its relationship with it's child node. For simplicity, I use a linear relationship shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Node:\n",
    "\n",
    "    def __init__(self, m:float=1, b:float=0) -> None:\n",
    "        self.slope = m\n",
    "        self.intercept = b\n",
    "        self.out = None\n",
    "\n",
    "    def propogate(self, input:np.array):\n",
    "        self.out = input * self.slope + self.intercept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3.66662417 2.17961209 1.57043584 1.11726065 2.50628165]\n",
      "[-0.73056406 -0.76323252 -0.71923614 -0.74444095 -0.78672049]\n"
     ]
    }
   ],
   "source": [
    "# define the parent nodes\n",
    "A = Node(m=3.1, b=1.1)\n",
    "B = Node(m=0.08, b=-0.79)\n",
    "\n",
    "# pass inputs to the parent nodes to determine the input for node V\n",
    "A.propogate(np.random.beta(1,1,5))\n",
    "B.propogate(np.random.beta(1,1,5))\n",
    "\n",
    "# view the outputs\n",
    "print(A.out)\n",
    "print(B.out)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the output above, the input to node $V$ would be somewhat difficult to define, but we can default this to be an additive relationship"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14.68030052  7.08189789  4.25599853  1.86409853  8.59780582]\n"
     ]
    }
   ],
   "source": [
    "# define node V\n",
    "V = Node(m=5, b=0)\n",
    "\n",
    "# define the input to V and propogate\n",
    "V_in = np.add(A.out, B.out)\n",
    "V.propogate(V_in)\n",
    "\n",
    "# print output\n",
    "print(V.out)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This short exploration very crudely defined a generative model to create causally-linked variables. To make this more complete, I would initialize $A$, $B$, and $V$ as Nodes in a graph which could then be used to generate values automatically with one propogation call. One thing that strikes me is the diversity of the distribution at the end of out the output from node $V$. Seeing as this generative model only has 3 nodes with fairly simple linear relationships, working backwards to regain the knowledge of slope/intercept seems much harder than using discrete variables. Hopefully, I can continue to explore this in another exploration."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
